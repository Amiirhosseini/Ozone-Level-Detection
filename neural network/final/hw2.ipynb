{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amirreza Hosseini** *9820363* <br>\n",
    "HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*reading the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.\\\\Ozone Level Detection.data', sep=',', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*handling the missing values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change every ? to random number\n",
    "df.replace('?', random(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*K fold cross validation* <br>\n",
    "partition df into 3 parts <br>\n",
    "train, test, validation <br>\n",
    "K=3 <br>\n",
    "70% for train <br>\n",
    "20% for test <br>\n",
    "10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[:int(len(df)*0.7)]\n",
    "test = df.iloc[int(len(df)*0.7):int(len(df)*0.9)]\n",
    "validation = df.iloc[int(len(df)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*extracting x & y from data* <br>\n",
    "also remove the date filed from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train.iloc[:,73:74]\n",
    "x_train=train.iloc[:, 1:-1]\n",
    "\n",
    "y_test=test.iloc[:,73:74]\n",
    "x_test=test.iloc[:, 1:-1]\n",
    "\n",
    "y_val=validation.iloc[:,73:74]\n",
    "x_val=validation.iloc[:, 1:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*convert the type of the data* <br>\n",
    "from object to float <br>\n",
    "for tensor parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1775, 1])\n",
      "torch.Size([1775, 72])\n"
     ]
    }
   ],
   "source": [
    "#convertion for x_test\n",
    "x_test=x_test.to_numpy()\n",
    "x_test=np.vstack(x_test).astype(np.float32) \n",
    "x_test=torch.from_numpy(x_test)\n",
    "\n",
    "#also do convertion for y_test\n",
    "y_test=y_test.to_numpy()\n",
    "y_test=np.vstack(y_test).astype(np.float32)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "\n",
    "#also do convertion for x_train\n",
    "x_train=x_train.to_numpy()\n",
    "x_train=np.vstack(x_train).astype(np.float32)\n",
    "x_train=torch.from_numpy(x_train)\n",
    "\n",
    "#also do convertion for y_train\n",
    "y_train=y_train.to_numpy()  \n",
    "y_train=np.vstack(y_train).astype(np.float32)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "\n",
    "#also do convertion for x_val\n",
    "x_val=x_val.to_numpy()\n",
    "x_val=np.vstack(x_val).astype(np.float32)\n",
    "x_val=torch.from_numpy(x_val)\n",
    "\n",
    "#also do convertion for y_val\n",
    "y_val=y_val.to_numpy()\n",
    "y_val=np.vstack(y_val).astype(np.float32)\n",
    "y_val=torch.from_numpy(y_val)\n",
    "\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**initial the neural network model** <br>\n",
    "4 fullly connected hidden layers <br>\n",
    "first activation function is RelU and the others are Sigmoid\n",
    "<br> showed in Forward function respectively.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.fc3 = torch.nn.Linear(self.hidden_size,self.hidden_size)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.fc4 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            hidden = self.fc2(relu)\n",
    "            relu = self.relu(hidden)\n",
    "            hidden=self.fc3(relu)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc4(relu)\n",
    "            output = self.sigmoid(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*create model instance* <br>\n",
    "loss function: BCE <br>\n",
    "optimizer: GD <br>\n",
    "learning rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(72, 50)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "losses_list1 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Go To Evaluation Mode* <br>\n",
    "test the loss before training with evaluation datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 0.7101112008094788\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_val)\n",
    "before_train = criterion(y_pred.squeeze(),y_val.squeeze()) \n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Go To Training Mode*\n",
    "for 70 epochs <br>\n",
    "print the loss in every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 1.2613261938095093\n",
      "Epoch 1: train loss: 1.2611732482910156\n",
      "Epoch 2: train loss: 1.2405903339385986\n",
      "Epoch 3: train loss: 0.6958646774291992\n",
      "Epoch 4: train loss: 0.6958626508712769\n",
      "Epoch 5: train loss: 0.695860743522644\n",
      "Epoch 6: train loss: 0.6958587169647217\n",
      "Epoch 7: train loss: 0.6958568096160889\n",
      "Epoch 8: train loss: 0.6958547830581665\n",
      "Epoch 9: train loss: 0.6958528161048889\n",
      "Epoch 10: train loss: 0.6958509683609009\n",
      "Epoch 11: train loss: 0.6958490014076233\n",
      "Epoch 12: train loss: 0.6958471536636353\n",
      "Epoch 13: train loss: 0.6958450675010681\n",
      "Epoch 14: train loss: 0.6958432197570801\n",
      "Epoch 15: train loss: 0.6958411931991577\n",
      "Epoch 16: train loss: 0.6958394050598145\n",
      "Epoch 17: train loss: 0.6958374977111816\n",
      "Epoch 18: train loss: 0.6958355903625488\n",
      "Epoch 19: train loss: 0.6958338022232056\n",
      "Epoch 20: train loss: 0.6958318948745728\n",
      "Epoch 21: train loss: 0.6958299875259399\n",
      "Epoch 22: train loss: 0.6958280801773071\n",
      "Epoch 23: train loss: 0.6958263516426086\n",
      "Epoch 24: train loss: 0.6958244442939758\n",
      "Epoch 25: train loss: 0.6958225965499878\n",
      "Epoch 26: train loss: 0.6958207488059998\n",
      "Epoch 27: train loss: 0.6958188414573669\n",
      "Epoch 28: train loss: 0.6958169937133789\n",
      "Epoch 29: train loss: 0.6958152651786804\n",
      "Epoch 30: train loss: 0.6958134174346924\n",
      "Epoch 31: train loss: 0.6958116292953491\n",
      "Epoch 32: train loss: 0.6958098411560059\n",
      "Epoch 33: train loss: 0.695807933807373\n",
      "Epoch 34: train loss: 0.6958062648773193\n",
      "Epoch 35: train loss: 0.6958044767379761\n",
      "Epoch 36: train loss: 0.695802628993988\n",
      "Epoch 37: train loss: 0.6958009004592896\n",
      "Epoch 38: train loss: 0.6957991123199463\n",
      "Epoch 39: train loss: 0.695797324180603\n",
      "Epoch 40: train loss: 0.6957955956459045\n",
      "Epoch 41: train loss: 0.6957939267158508\n",
      "Epoch 42: train loss: 0.6957921981811523\n",
      "Epoch 43: train loss: 0.6957904696464539\n",
      "Epoch 44: train loss: 0.6957886815071106\n",
      "Epoch 45: train loss: 0.6957869529724121\n",
      "Epoch 46: train loss: 0.6957852244377136\n",
      "Epoch 47: train loss: 0.6957834362983704\n",
      "Epoch 48: train loss: 0.6957818269729614\n",
      "Epoch 49: train loss: 0.6957800984382629\n",
      "Epoch 50: train loss: 0.6957783699035645\n",
      "Epoch 51: train loss: 0.695776641368866\n",
      "Epoch 52: train loss: 0.695775032043457\n",
      "Epoch 53: train loss: 0.6957734227180481\n",
      "Epoch 54: train loss: 0.6957716345787048\n",
      "Epoch 55: train loss: 0.6957699656486511\n",
      "Epoch 56: train loss: 0.6957682967185974\n",
      "Epoch 57: train loss: 0.6957666873931885\n",
      "Epoch 58: train loss: 0.6957650184631348\n",
      "Epoch 59: train loss: 0.695763349533081\n",
      "Epoch 60: train loss: 0.6957617998123169\n",
      "Epoch 61: train loss: 0.695760190486908\n",
      "Epoch 62: train loss: 0.695758581161499\n",
      "Epoch 63: train loss: 0.6957569122314453\n",
      "Epoch 64: train loss: 0.6957552433013916\n",
      "Epoch 65: train loss: 0.6957535743713379\n",
      "Epoch 66: train loss: 0.6957520842552185\n",
      "Epoch 67: train loss: 0.6957505345344543\n",
      "Epoch 68: train loss: 0.6957489252090454\n",
      "Epoch 69: train loss: 0.6957472562789917\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 70\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train.squeeze())\n",
    "    losses_list1.append(loss.item())\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Go To Evaluation Mode* <br>\n",
    "test the loss after training with test datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.6941480040550232\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test.squeeze()) \n",
    "print('Test loss after Training' , after_train.item())\n",
    "losses_list2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*calculate the delta of loss* <br>\n",
    "before and after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute difference between before and after training:  0.011317312717437744\n"
     ]
    }
   ],
   "source": [
    "delta=after_train.item()-before_train.item()\n",
    "#print absoluted difference\n",
    "print('Absolute difference between before and after training: ', abs(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*calculate Standard deviation and avrage* <br>\n",
    "for test and validation data for new learning rate=0.09 <br>\n",
    "and 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.6937658786773682\n",
      "Standard deviation of after_train and before_train:  0.0\n",
      "Average of after_train and before_train:  1.403877079486847\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.09)\n",
    "\n",
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train.squeeze())\n",
    "    losses_list2.append(loss.item())\n",
    "   \n",
    "    #print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test.squeeze()) \n",
    "print('Test loss after Training' , after_train.item())\n",
    "\n",
    "#print standard deviation of after_train and before_train\n",
    "print('Standard deviation of after_train and before_train: ', np.std(after_train.item()+before_train.item()))\n",
    "\n",
    "#print average of after_train and before_train\n",
    "print('Average of after_train and before_train: ', np.mean(after_train.item()+before_train.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*find model overfitting or underfitting* <br>\n",
    "if model overfitting then loss will be high <br>\n",
    "if model underfitting then loss will be low <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot losses_list1 and losses_list2 with labels\n",
    "from matplotlib import pyplot as plt    \n",
    "plt.plot(losses_list1, label='before training')\n",
    "plt.plot(losses_list2, label='after training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ff9d546f78a45e6703bb7c09f46b817ecce56a1eb2e1f2195fbbfca60f66129"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
